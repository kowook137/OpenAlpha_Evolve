"""
Multi-Metric Evaluator for AlphaEvolve
ë‹¤ì°¨ì› í‰ê°€ ì‹œìŠ¤í…œ - ì½”ë“œ í’ˆì§ˆ, íš¨ìœ¨ì„±, ë³µì¡ë„ ë“±ì„ ì¢…í•© í‰ê°€
"""
import ast
import time
import logging
import asyncio
import radon.complexity as radon
from typing import Dict, Any, Optional, List, Tuple
from dataclasses import dataclass

from core.interfaces import Program, TaskDefinition
from config import settings

logger = logging.getLogger(__name__)

@dataclass
class CodeMetrics:
    """ì½”ë“œ ë©”íŠ¸ë¦­ ë°ì´í„° í´ë˜ìŠ¤"""
    cyclomatic_complexity: float
    lines_of_code: int
    execution_time_ms: float
    memory_usage_mb: float
    operation_count: int
    readability_score: float
    mathematical_elegance: float
    efficiency_score: float

class MultiMetricEvaluator:
    """ë‹¤ì°¨ì› í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self, llm_provider: Optional[str] = None):
        self.llm_provider = llm_provider or settings.EVALUATION_KEY
        self.weights = {
            'correctness': 0.4,      # ê¸°ëŠ¥ì  ì •í™•ì„± (ê°€ì¥ ì¤‘ìš”)
            'efficiency': 0.2,       # ì—°ì‚° íš¨ìœ¨ì„±
            'complexity': 0.15,      # ì½”ë“œ ë³µì¡ë„ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
            'readability': 0.1,      # ê°€ë…ì„±
            'elegance': 0.1,         # ìˆ˜í•™ì  ìš°ì•„í•¨
            'performance': 0.05      # ì‹¤í–‰ ì„±ëŠ¥
        }
        
    def calculate_cyclomatic_complexity(self, code: str) -> float:
        """ìˆœí™˜ ë³µì¡ë„ ê³„ì‚°"""
        try:
            tree = ast.parse(code)
            complexity_blocks = radon.cc_visit(tree)
            
            if not complexity_blocks:
                return 1.0
            
            total_complexity = sum(block.complexity for block in complexity_blocks)
            avg_complexity = total_complexity / len(complexity_blocks)
            
            # ì •ê·œí™”: 1-10 ë²”ìœ„ë¡œ ë³€í™˜
            normalized = min(avg_complexity, 10.0)
            return normalized
            
        except Exception as e:
            logger.warning(f"ë³µì¡ë„ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 5.0  # ê¸°ë³¸ê°’
    
    def count_operations(self, code: str) -> int:
        """ì½”ë“œ ë‚´ ì—°ì‚° íšŸìˆ˜ ì¶”ì •"""
        try:
            tree = ast.parse(code)
            operation_count = 0
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.BinOp, ast.UnaryOp, ast.Compare)):
                    operation_count += 1
                elif isinstance(node, ast.Call):
                    operation_count += 2  # í•¨ìˆ˜ í˜¸ì¶œì€ ë” ë¹„ì‹¼ ì—°ì‚°
                elif isinstance(node, (ast.For, ast.While)):
                    operation_count += 5  # ë°˜ë³µë¬¸ì€ ë³µí•© ì—°ì‚°
                elif isinstance(node, ast.ListComp):
                    operation_count += 3  # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜
            
            return operation_count
            
        except Exception as e:
            logger.warning(f"ì—°ì‚° íšŸìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 100  # ê¸°ë³¸ê°’
    
    def calculate_lines_of_code(self, code: str) -> int:
        """ìœ íš¨ ì½”ë“œ ë¼ì¸ ìˆ˜ ê³„ì‚° (ì£¼ì„, ë¹ˆ ì¤„ ì œì™¸)"""
        lines = code.split('\n')
        effective_lines = 0
        
        for line in lines:
            stripped = line.strip()
            if stripped and not stripped.startswith('#'):
                effective_lines += 1
        
        return effective_lines
    
    async def evaluate_readability(self, code: str) -> float:
        """LLMì„ í†µí•œ ì½”ë“œ ê°€ë…ì„± í‰ê°€"""
        try:
            from code_generator.agent import CodeGeneratorAgent
            
            generator = CodeGeneratorAgent()
            
            prompt = f"""
Rate the readability of this Python code on a scale of 1-10, where:
- 1 = Very hard to read (poor naming, no structure)
- 5 = Average readability
- 10 = Excellent readability (clear naming, good structure, well-organized)

Consider:
- Variable and function naming
- Code structure and organization
- Use of comments and docstrings
- Overall clarity

Code to evaluate:
```python
{code}
```

Respond with only a single number between 1 and 10.
"""
            
            response = await generator.generate_code(
                prompt=prompt,
                temperature=0.3,
                output_format="code"
            )
            
            # ìˆ«ì ì¶”ì¶œ
            try:
                score = float(response.strip())
                return max(1.0, min(10.0, score))
            except:
                return 5.0  # ê¸°ë³¸ê°’
                
        except Exception as e:
            logger.warning(f"ê°€ë…ì„± í‰ê°€ ì‹¤íŒ¨: {e}")
            return 5.0
    
    async def evaluate_mathematical_elegance(self, code: str, task: TaskDefinition) -> float:
        """LLMì„ í†µí•œ ìˆ˜í•™ì  ìš°ì•„í•¨ í‰ê°€"""
        try:
            from code_generator.agent import CodeGeneratorAgent
            
            generator = CodeGeneratorAgent()
            
            prompt = f"""
Rate the mathematical elegance of this algorithm for {task.description} on a scale of 1-10, where:
- 1 = Brute force, no mathematical insight
- 5 = Standard algorithmic approach
- 10 = Highly elegant, uses advanced mathematical concepts

Consider:
- Use of mathematical structures (group theory, finite fields, etc.)
- Algorithmic sophistication
- Theoretical foundations
- Innovation in approach

Code to evaluate:
```python
{code}
```

Respond with only a single number between 1 and 10.
"""
            
            response = await generator.generate_code(
                prompt=prompt,
                temperature=0.3,
                output_format="code"
            )
            
            try:
                score = float(response.strip())
                return max(1.0, min(10.0, score))
            except:
                return 5.0
                
        except Exception as e:
            logger.warning(f"ìˆ˜í•™ì  ìš°ì•„í•¨ í‰ê°€ ì‹¤íŒ¨: {e}")
            return 5.0
    
    def calculate_efficiency_score(self, operation_count: int, lines_of_code: int) -> float:
        """íš¨ìœ¨ì„± ì ìˆ˜ ê³„ì‚° (ì—°ì‚° ëŒ€ë¹„ ì½”ë“œ ê¸¸ì´)"""
        if lines_of_code == 0:
            return 1.0
        
        # ì—°ì‚° ë°€ë„ ê³„ì‚° (ë¼ì¸ë‹¹ ì—°ì‚° ìˆ˜)
        operation_density = operation_count / lines_of_code
        
        # ì •ê·œí™”: ë†’ì€ ë°€ë„ì¼ìˆ˜ë¡ ë¹„íš¨ìœ¨ì 
        # ì ì ˆí•œ ë°€ë„ëŠ” 2-5 ì •ë„ë¡œ ê°€ì •
        if operation_density <= 2:
            return 10.0
        elif operation_density <= 5:
            return 8.0 - (operation_density - 2) * 2
        else:
            return max(1.0, 8.0 - (operation_density - 5) * 0.5)
    
    async def evaluate_comprehensive(
        self, 
        program: Program, 
        task: TaskDefinition,
        execution_time_ms: float,
        memory_usage_mb: float,
        correctness_score: float
    ) -> Dict[str, Any]:
        """ì¢…í•©ì  ë‹¤ì°¨ì› í‰ê°€"""
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        complexity = self.calculate_cyclomatic_complexity(program.code)
        lines_of_code = self.calculate_lines_of_code(program.code)
        operation_count = self.count_operations(program.code)
        efficiency_score = self.calculate_efficiency_score(operation_count, lines_of_code)
        
        # LLM ê¸°ë°˜ í‰ê°€ (ë³‘ë ¬ ì‹¤í–‰)
        readability_task = self.evaluate_readability(program.code)
        elegance_task = self.evaluate_mathematical_elegance(program.code, task)
        
        readability_score, elegance_score = await asyncio.gather(
            readability_task, elegance_task
        )
        
        # ë©”íŠ¸ë¦­ ì •ê·œí™” (1-10 ìŠ¤ì¼€ì¼)
        normalized_metrics = {
            'correctness': correctness_score * 10,  # 0-1 -> 0-10
            'efficiency': efficiency_score,         # ì´ë¯¸ 1-10
            'complexity': max(1, 11 - complexity), # ë³µì¡ë„ëŠ” ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ
            'readability': readability_score,       # ì´ë¯¸ 1-10
            'elegance': elegance_score,            # ì´ë¯¸ 1-10
            'performance': max(1, 11 - min(10, execution_time_ms / 100))  # ì‹œê°„ ê¸°ë°˜
        }
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_score = sum(
            normalized_metrics[metric] * self.weights[metric]
            for metric in self.weights.keys()
        )
        
        # ìƒì„¸ ë©”íŠ¸ë¦­ ì •ë³´
        detailed_metrics = {
            'cyclomatic_complexity': complexity,
            'lines_of_code': lines_of_code,
            'operation_count': operation_count,
            'execution_time_ms': execution_time_ms,
            'memory_usage_mb': memory_usage_mb,
            'readability_score': readability_score,
            'mathematical_elegance': elegance_score,
            'efficiency_score': efficiency_score,
            'normalized_metrics': normalized_metrics,
            'weighted_total_score': weighted_score,
            'metric_weights': self.weights
        }
        
        logger.info(f"ë‹¤ì°¨ì› í‰ê°€ ì™„ë£Œ - ì´ì : {weighted_score:.2f}/10")
        logger.info(f"ì„¸ë¶€ ì ìˆ˜: {normalized_metrics}")
        
        return detailed_metrics
    
    def compare_programs(self, prog1_metrics: Dict, prog2_metrics: Dict) -> Dict[str, Any]:
        """ë‘ í”„ë¡œê·¸ë¨ì˜ ë©”íŠ¸ë¦­ ë¹„êµ"""
        comparison = {}
        
        for metric in self.weights.keys():
            score1 = prog1_metrics['normalized_metrics'][metric]
            score2 = prog2_metrics['normalized_metrics'][metric]
            
            comparison[f"{metric}_improvement"] = score2 - score1
            comparison[f"{metric}_improvement_pct"] = ((score2 - score1) / score1) * 100 if score1 > 0 else 0
        
        total_improvement = prog2_metrics['weighted_total_score'] - prog1_metrics['weighted_total_score']
        comparison['total_improvement'] = total_improvement
        comparison['is_better_overall'] = total_improvement > 0
        
        return comparison

# ì‚¬ìš© ì˜ˆì‹œ
async def example_usage():
    """ë‹¤ì°¨ì› í‰ê°€ ì‹œìŠ¤í…œ ì‚¬ìš© ì˜ˆì‹œ"""
    evaluator = MultiMetricEvaluator()
    
    # ê°€ìƒì˜ í”„ë¡œê·¸ë¨ê³¼ íƒœìŠ¤í¬
    program = Program(
        id="test_program",
        code="""
def solve():
    # GF(8) êµ¬í˜„
    n = 8
    latin1 = [[i ^ j for j in range(n)] for i in range(n)]
    latin2 = [[i ^ (j * 2) for j in range(n)] for i in range(n)]
    return latin1, latin2
        """,
        generation=1
    )
    
    task = TaskDefinition(
        id="mols_8x8",
        description="Generate 8x8 Mutually Orthogonal Latin Squares",
        input_output_examples=[],
        evaluation_criteria={}
    )
    
    # ì¢…í•© í‰ê°€ ì‹¤í–‰
    metrics = await evaluator.evaluate_comprehensive(
        program=program,
        task=task,
        execution_time_ms=50.0,
        memory_usage_mb=2.5,
        correctness_score=0.95
    )
    
    print("ğŸ“Š ë‹¤ì°¨ì› í‰ê°€ ê²°ê³¼:")
    print(f"ì´ì : {metrics['weighted_total_score']:.2f}/10")
    print(f"ì„¸ë¶€ ì ìˆ˜: {metrics['normalized_metrics']}")

if __name__ == "__main__":
    asyncio.run(example_usage()) 