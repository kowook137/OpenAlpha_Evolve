"""
ë‹¤ì°¨ì› í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
AlphaEvolveì˜ ìƒˆë¡œìš´ Multi-Metric Evaluation ì‹œìŠ¤í…œì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤
"""
import asyncio
import logging
from typing import List

from core.interfaces import Program, TaskDefinition
from evaluator_agent.multi_metric_evaluator import MultiMetricEvaluator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_test_programs() -> List[Program]:
    """ë‹¤ì–‘í•œ ë³µì¡ë„ì˜ í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ë“¤ ìƒì„±"""
    
    # 1. ê°„ë‹¨í•œ ë¸Œë£¨íŠ¸ í¬ìŠ¤ ì ‘ê·¼ë²•
    simple_code = """
def solve():
    # ê°„ë‹¨í•œ ë¸Œë£¨íŠ¸ í¬ìŠ¤ ì ‘ê·¼
    n = 8
    square1 = []
    square2 = []
    
    for i in range(n):
        row1 = []
        row2 = []
        for j in range(n):
            row1.append((i + j) % n)
            row2.append((i * 2 + j) % n)
        square1.append(row1)
        square2.append(row2)
    
    return square1, square2
"""
    
    # 2. ìœ í•œì²´ ê¸°ë°˜ ìš°ì•„í•œ ì ‘ê·¼ë²• (ì œê³µí•´ì£¼ì‹  ì½”ë“œ)
    elegant_code = """
import numpy as np

# GF(8) ìƒì„±: x^3 + x + 1 => binary 1011
POLY = 0b1011  # GF(2^3)ì—ì„œì˜ í™˜ì‚°ìš© ë‹¤í•­ì‹

def gf8_add(a, b):
    \"\"\"GF(8)ì—ì„œ ë§ì…ˆì€ ë‹¨ìˆœ XOR\"\"\"
    return a ^ b

def gf8_mult(a, b):
    \"\"\"GF(8)ì—ì„œ ê³±ì…ˆ êµ¬í˜„: ë‹¤í•­ì‹ ê³± í›„ mod (x^3 + x + 1)\"\"\"
    result = 0
    temp_a = a
    i = 0
    while temp_a:
        if temp_a & 1:
            result ^= (b << i)
        temp_a >>= 1
        i += 1
    # ê²°ê³¼ ë‹¤í•­ì‹ì„ POLYë¡œ ë‚˜ëˆ ì„œ í™˜ì‚°
    if result & 0b10000:
        result ^= (POLY << 1)
    if result & 0b1000:
        result ^= POLY
    return result  # ê²°ê³¼ëŠ” 0~7 ì‚¬ì´ ê°’

def solve():
    # ë¹„-trivialí•œ ìƒì„±ì› Î± = x ë¥¼ ì„ íƒ
    alpha = 0b010  # GF(8)ì—ì„œ xì— í•´ë‹¹ (binary)
    
    n = 8
    latin1 = [[gf8_add(r, c) for c in range(n)] for r in range(n)]
    latin2 = [[gf8_add(r, gf8_mult(alpha, c)) for c in range(n)] for r in range(n)]
    
    return latin1, latin2
"""
    
    # 3. ë³µì¡í•˜ì§€ë§Œ ë¹„íš¨ìœ¨ì ì¸ ì ‘ê·¼ë²•
    complex_code = """
import itertools
import random
import time

def solve():
    \"\"\"ë³µì¡í•˜ì§€ë§Œ ë¹„íš¨ìœ¨ì ì¸ MOLS ìƒì„±\"\"\"
    n = 8
    
    # ë³µì¡í•œ ì´ˆê¸°í™”
    base_values = list(range(n))
    permutations = list(itertools.permutations(base_values))
    
    # ë¹„íš¨ìœ¨ì ì¸ ì¤‘ì²© ë°˜ë³µë¬¸
    best_square1 = None
    best_square2 = None
    best_score = -1
    
    for attempt in range(100):  # ë§ì€ ì‹œë„
        square1 = []
        square2 = []
        
        for i in range(n):
            row1 = []
            row2 = []
            for j in range(n):
                # ë³µì¡í•œ ê³„ì‚°
                val1 = 0
                val2 = 0
                for k in range(10):  # ë¶ˆí•„ìš”í•œ ë°˜ë³µ
                    temp = (i + j + k) % n
                    val1 ^= temp
                    val2 ^= (temp * 2) % n
                
                row1.append(val1 % n)
                row2.append(val2 % n)
            
            square1.append(row1)
            square2.append(row2)
        
        # ë¬´ì˜ë¯¸í•œ ì ìˆ˜ ê³„ì‚°
        score = sum(sum(row) for row in square1) + sum(sum(row) for row in square2)
        if score > best_score:
            best_score = score
            best_square1 = square1
            best_square2 = square2
    
    return best_square1, best_square2
"""
    
    # 4. ë§¤ìš° ê°„ê²°í•œ ì ‘ê·¼ë²•
    concise_code = """
def solve():
    n = 8
    return [[(i+j)%n for j in range(n)] for i in range(n)], [[(i*2+j)%n for j in range(n)] for i in range(n)]
"""
    
    programs = [
        Program(id="simple_bruteforce", code=simple_code, generation=1),
        Program(id="elegant_galois_field", code=elegant_code, generation=1),
        Program(id="complex_inefficient", code=complex_code, generation=1),
        Program(id="concise_oneliner", code=concise_code, generation=1)
    ]
    
    return programs

async def test_multi_metric_evaluation():
    """ë‹¤ì°¨ì› í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ë‹¤ì°¨ì› í‰ê°€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹œì‘\n")
    
    # í…ŒìŠ¤íŠ¸ í”„ë¡œê·¸ë¨ë“¤ ìƒì„±
    programs = create_test_programs()
    
    # íƒœìŠ¤í¬ ì •ì˜
    task = TaskDefinition(
        id="mols_8x8",
        description="Generate 8x8 Mutually Orthogonal Latin Squares",
        input_output_examples=[],
        evaluation_criteria={}
    )
    
    # ë‹¤ì°¨ì› í‰ê°€ì ì´ˆê¸°í™”
    evaluator = MultiMetricEvaluator()
    
    print("ğŸ“Š ê° í”„ë¡œê·¸ë¨ë³„ ë‹¤ì°¨ì› í‰ê°€ ê²°ê³¼:\n")
    
    results = []
    
    for program in programs:
        print(f"ğŸ” í‰ê°€ ì¤‘: {program.id}")
        print(f"ì½”ë“œ ê¸¸ì´: {len(program.code)} ë¬¸ì")
        
        try:
            # ê°€ìƒì˜ ì‹¤í–‰ ê²°ê³¼ (ì‹¤ì œë¡œëŠ” evaluator_agentì—ì„œ ì‹¤í–‰)
            execution_time = {
                "simple_bruteforce": 45.0,
                "elegant_galois_field": 25.0,
                "complex_inefficient": 150.0,
                "concise_oneliner": 20.0
            }.get(program.id, 50.0)
            
            memory_usage = {
                "simple_bruteforce": 2.1,
                "elegant_galois_field": 2.5,
                "complex_inefficient": 4.2,
                "concise_oneliner": 1.8
            }.get(program.id, 2.0)
            
            correctness = {
                "simple_bruteforce": 0.85,
                "elegant_galois_field": 0.95,
                "complex_inefficient": 0.75,
                "concise_oneliner": 0.90
            }.get(program.id, 0.8)
            
            # ë‹¤ì°¨ì› í‰ê°€ ì‹¤í–‰
            metrics = await evaluator.evaluate_comprehensive(
                program=program,
                task=task,
                execution_time_ms=execution_time,
                memory_usage_mb=memory_usage,
                correctness_score=correctness
            )
            
            results.append((program.id, metrics))
            
            print(f"âœ… ì¢…í•© ì ìˆ˜: {metrics['weighted_total_score']:.2f}/10")
            print(f"   - ì •í™•ì„±: {metrics['normalized_metrics']['correctness']:.1f}/10")
            print(f"   - íš¨ìœ¨ì„±: {metrics['normalized_metrics']['efficiency']:.1f}/10")
            print(f"   - ë³µì¡ë„: {metrics['normalized_metrics']['complexity']:.1f}/10")
            print(f"   - ê°€ë…ì„±: {metrics['normalized_metrics']['readability']:.1f}/10")
            print(f"   - ìš°ì•„í•¨: {metrics['normalized_metrics']['elegance']:.1f}/10")
            print(f"   - ì„±ëŠ¥: {metrics['normalized_metrics']['performance']:.1f}/10")
            print(f"   - ì½”ë“œ ë¼ì¸ ìˆ˜: {metrics['lines_of_code']}")
            print(f"   - ìˆœí™˜ ë³µì¡ë„: {metrics['cyclomatic_complexity']:.1f}")
            print(f"   - ì—°ì‚° íšŸìˆ˜: {metrics['operation_count']}")
            print()
            
        except Exception as e:
            print(f"âŒ í‰ê°€ ì‹¤íŒ¨: {e}\n")
    
    # ê²°ê³¼ ë¹„êµ ë° ìˆœìœ„
    print("ğŸ† í”„ë¡œê·¸ë¨ ìˆœìœ„ (ì¢…í•© ì ìˆ˜ ê¸°ì¤€):")
    print("=" * 60)
    
    sorted_results = sorted(results, key=lambda x: x[1]['weighted_total_score'], reverse=True)
    
    for i, (program_id, metrics) in enumerate(sorted_results, 1):
        print(f"{i}. {program_id}")
        print(f"   ì¢…í•© ì ìˆ˜: {metrics['weighted_total_score']:.2f}/10")
        print(f"   íŠ¹ì§•: ", end="")
        
        # ê° í”„ë¡œê·¸ë¨ì˜ ê°•ì  ë¶„ì„
        strengths = []
        if metrics['normalized_metrics']['correctness'] >= 9:
            strengths.append("ë†’ì€ ì •í™•ì„±")
        if metrics['normalized_metrics']['efficiency'] >= 8:
            strengths.append("ìš°ìˆ˜í•œ íš¨ìœ¨ì„±")
        if metrics['normalized_metrics']['complexity'] >= 8:
            strengths.append("ë‚®ì€ ë³µì¡ë„")
        if metrics['normalized_metrics']['elegance'] >= 8:
            strengths.append("ìˆ˜í•™ì  ìš°ì•„í•¨")
        if metrics['normalized_metrics']['readability'] >= 8:
            strengths.append("ë†’ì€ ê°€ë…ì„±")
        
        print(", ".join(strengths) if strengths else "ê°œì„  í•„ìš”")
        print()
    
    # í”„ë¡œê·¸ë¨ ê°„ ë¹„êµ
    if len(sorted_results) >= 2:
        best = sorted_results[0]
        second = sorted_results[1]
        
        comparison = evaluator.compare_programs(second[1], best[1])
        
        print("ğŸ”„ 1ìœ„ì™€ 2ìœ„ í”„ë¡œê·¸ë¨ ë¹„êµ:")
        print(f"{best[0]} vs {second[0]}")
        print(f"ì´ì  ê°œì„ : {comparison['total_improvement']:.2f}ì ")
        print(f"ì •í™•ì„± ê°œì„ : {comparison['correctness_improvement']:.2f}ì ")
        print(f"íš¨ìœ¨ì„± ê°œì„ : {comparison['efficiency_improvement']:.2f}ì ")
        print(f"ë³µì¡ë„ ê°œì„ : {comparison['complexity_improvement']:.2f}ì ")

async def test_metric_weights():
    """ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜ ì¡°ì • í…ŒìŠ¤íŠ¸"""
    print("\nğŸ›ï¸ ë©”íŠ¸ë¦­ ê°€ì¤‘ì¹˜ ì¡°ì • í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # ë‹¤ë¥¸ ê°€ì¤‘ì¹˜ ì„¤ì •ë“¤
    weight_configs = [
        {
            'name': 'ì •í™•ì„± ì¤‘ì‹¬',
            'weights': {
                'correctness': 0.6,
                'efficiency': 0.15,
                'complexity': 0.1,
                'readability': 0.05,
                'elegance': 0.05,
                'performance': 0.05
            }
        },
        {
            'name': 'íš¨ìœ¨ì„± ì¤‘ì‹¬',
            'weights': {
                'correctness': 0.3,
                'efficiency': 0.3,
                'complexity': 0.2,
                'readability': 0.05,
                'elegance': 0.05,
                'performance': 0.1
            }
        },
        {
            'name': 'ìš°ì•„í•¨ ì¤‘ì‹¬',
            'weights': {
                'correctness': 0.3,
                'efficiency': 0.15,
                'complexity': 0.15,
                'readability': 0.15,
                'elegance': 0.2,
                'performance': 0.05
            }
        }
    ]
    
    programs = create_test_programs()
    task = TaskDefinition(id="test", description="Test task", input_output_examples=[], evaluation_criteria={})
    
    for config in weight_configs:
        print(f"\nğŸ“Š {config['name']} ê°€ì¤‘ì¹˜ ì ìš©:")
        
        evaluator = MultiMetricEvaluator()
        evaluator.weights = config['weights']
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ (ì²« ë²ˆì§¸ í”„ë¡œê·¸ë¨ë§Œ)
        program = programs[1]  # elegant_galois_field
        
        metrics = await evaluator.evaluate_comprehensive(
            program=program,
            task=task,
            execution_time_ms=25.0,
            memory_usage_mb=2.5,
            correctness_score=0.95
        )
        
        print(f"ì¢…í•© ì ìˆ˜: {metrics['weighted_total_score']:.2f}/10")
        print(f"ê°€ì¤‘ì¹˜: {config['weights']}")

if __name__ == "__main__":
    asyncio.run(test_multi_metric_evaluation())
    asyncio.run(test_metric_weights()) 